---
title: Quiz
---

## Question 4

멀티파트 업로드를 사용해 S3 버킷으로 크기가 큰 파일을 업로드하던 도중, 네트워크 문제로 인해 완료되지 않은 부분의 상당량이 S3 버킷에 저장되었습니다. 완료되지 않은 부분은 불필요하며, 여러분의 비용을 소모합니다. 완료되지 않은 부분을 삭제하기 위해 사용할 수 있는 최적의 접근법은 무엇인가요?

- S3 수명 주기 정책을 사용해 오래된/완료되지 않은 부분의 삭제를 자동화


## Question 6

Amazon RDS PostgreSQL을 사용하여 S3에 파일의 인덱스를 구축하려 합니다. 인덱스 구축을 위해서는 파일 콘텐츠 자체의 메타데이터를 포함하고 있는 S3에 있는 각 객체의 첫 250바이트를 읽는 작업이 필수적입니다. S3 버킷에는 총 50TB에 달하는 100,000개 이상의 파일이 포함되어 있습니다. 이 경우 인덱스를 구축하기 위한 효율적인 방법은 무엇일까요?

- S3 버킷을 트래버스하고, 첫 250바이트에 대한 바이트 범위 페치를 발행한 후, RDS에 해당 정보를 저장하는 애플리케이션 생성


## Question 7

S3 버킷으로 업로드하려는 대규모의 데이터셋이 온프레미스로 저장되어 있습니다. 이 데이터셋은 10GB 파일로 분할되어 있습니다. 대역폭은 좋으나, 인터넷 연결은 불안정합니다. 이 경우, 데이터셋을 S3에 업로드하는 프로세스를 빠르게, 그리고 인터넷 연결 문제 없이 처리하려면 어떤 방법을 사용해야 할까요?

- S3 멀티파트 업로드와 S3 전송 가속화 사용


## Question 9

회사에서 AWS의 인트라넷 규정 준수 및 규제 검토를 준비하고 있습니다. 회사는 현재 암호화되지 않은 S3 버킷에 파일을 저장하고 있으며, 규정 준수 및 규제 검토에 필요할 경우 해당 파일은 반드시 암호화해야 합니다. 다음 중 S3 버킷의 모든 파일을 가장 효율적이고 경제적인 방식으로 암호화할 수 있는 S3 기능은 무엇입니까?

- S3 Batch Operation
